---
title: "Group4_Project4"
author: "Brandon Croarkin, Justin Longo, Brian Peirce, Teng Siong (T.S) Yeap"
output: 
  flexdashboard::flex_dashboard:
    storyboard : TRUE
---

```{r setup, include=FALSE}
library(flexdashboard)
```

###The 'R' skills needed and the function explainations
**'R' skills:**

- *ggplot2*, *plotly* and *scales* library allows us to build complex visualizations that will aid the generation of further insights.
psych library helps us to explore the interactions among data through scatter plots and histograms.

- *QRM* estimates Student-t and generalized pareto distribution (GPD) simulation.

- *quantreg* to estimation and inference methods for models of conditional quantiles

- *flexdashboard* to publish a group of related data visualizations as a dashboard. It allows for storyboard layouts for presenting sequences of visualizations and related commentary. 

- *shiny* to create interactive visualizations of data. 

- *tidyr* helps for data tidying to help clean and reshape data for analysis

- *zoo* is used to help with irregular time series of numeric vectors/matrices and factors

- *psych* library helps us to explore the interactions among data through scatter plots and histograms.

**Function explanations:**

- *read.csv()*: reads in the comma separated file with data for nickel, copper, and aluminum

- *na.omit()*: returns a data table with just the rows where the specified columns have no missing value in any of them. 

- *as.matrix()*: coerces to a matrix object

- *ifelse()*: returns a value with the same shape as the vector it is used on filled with elements depending on whether the test condition is TRUE or FALSE. Was used to add in a direction column for whether the return is positive or negative. 

- *diff()*: returns suitably lagged and iterated differences

- *log()*: computes logarithms, by default natural logarithms

- *abs()*: returns absolute values

- *colnames()*: sets the column names of a matrix-like object

- *paste()*: concatenates vectors after converting to character

- *as.Date()*: converts between character representations and objects of class "Date" representing calendar dates

- *cbind()*: takes a vector, matrix, or dataframe and combines by columns

- *as.character()*: create or test for objects of type "character"

- *data.frame()*: creates data frames with tightly coupled collections of variables

- *as.zooreg()*: a subclass of "zoo" that is used to represent both weakly and strictly regular series.

- *data_moments()*: using the "moments" and "matrixStats" libraries it creates a dataframe with a set of common data summarization methods (means, median, standard deviation, IQR, skewness, and kurtosis)

- *quantile()*: produces sample quantiles corresponding to the given probabilities

- *subset()*: returns a subset of the returns (R) dataframe with nickel, copper, and aluminum returns where nickel is greater than the 95th quantile of nickel

- *apply()*: returns a vector or list of values by applying a function to an array or matrix. Used to get the column means of the subsetted R dataframe. 

- *seq()*: 
    1. generates a sequence of 300 numbers from .5 of the min of mean.R        to 1.5 of the max of mean.R
    2. Generates a sequence of 300 numbers from the min of mean.R +            .0001 to the max of mean.R - .0001

    
- *matrix()*: used to make an empty matrix with 300 rows and 3 columns and uses a for loop to populate the matrix with the solution of solve.QP

- *solve.QP()*: implements the dual method of Goldfarb and Idnani for solving quadratic programming problems 

###Exploratory Analysis
```{r}
rm(list = ls())

library(ggplot2)
library(flexdashboard)
library(shiny)
library(QRM) #GPD fit
library(qrmdata)
library(xts)
library(zoo)
library(psych)
library(quadprog)
library(matrixStats)
library(quantreg)
library(moments)
library(plotly)
library(mvtnorm)
library(reshape2)

data <- na.omit(read.csv(url("https://turing.manhattan.edu/~wfoote01/finalytics/data/metaldata.csv"), header = TRUE))
# Compute log differences percent using as.matrix to force numeric type
data.r <- diff(log(as.matrix(data[, -1]))) * 100
# Create size and direction
size <- na.omit(abs(data.r)) # size is indicator of volatility
#head(size)
colnames(size) <- paste(colnames(size),".size", sep = "") # Teetor
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0)) # another indicator of skewness
colnames(direction) <- paste(colnames(direction),".dir", sep = "")
# Convert into a time series object: 
# 1. Split into date and rates
dates <- as.Date(data$DATE[-1], "%m/%d/%Y")
dates.chr <- as.character(data$DATE[-1])
str(dates.chr)
values <- cbind(data.r, size, direction)
# for dplyr pivoting and ggplot2 need a data frame also known as "tidy data"
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE) 
#non-coerced dates for subsetting on non-date columns
# 2. Make an xts object with row names equal to the dates
data.xts <- na.omit(as.xts(values, dates)) #order.by=as.Date(dates, "%d/%m/%Y")))
#str(data.xts)
data.zr <- as.zooreg(data.xts)
returns <- data.xts

##
# Load the data_moments() function
## data_moments function
## INPUTS: r vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  library(matrixStats)
  mean.r <- colMeans(data)
  median.r <- colMedians(data)
  sd.r <- colSds(data)
  IQR.r <- colIQRs(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, median = median.r, std_dev = sd.r, IQR = IQR.r, skewness = skewness.r, kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(data.xts[, 1:3])
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)
```

###Portfolio Analytics: the Markowitz model 

```{r}
R <- returns[,1:3]/100
quantile_R <- quantile(R[,1], 0.95) # look at tail of the nickel distribution
R <- subset(R, nickel > quantile_R, select = nickel:aluminium)
names.R <- colnames(R)
mean.R <-  apply(R,2,mean)
cov.R <-  cov(R)
sd.R <-  sqrt(diag(cov.R)) ## remember these are in daily percentages
#library(quadprog)
Amat <-  cbind(rep(1,3),mean.R)  ## set the equality constraints matrix
mu.P <- seq(0.5*min(mean.R), 1.5*max(mean.R), length = 300)  ## set of 300 possible target portfolio returns
#mu.P <- seq(0.5*quantile_R, max(R), length = 100)  ## set of 300 possible target portfolio returns
sigma.P <-  mu.P ## set up storage for std dev's of portfolio returns
weights <-  matrix(0, nrow=300, ncol = ncol(R)) ## storage for portfolio weights
colnames(weights) <- names.R
for (i in 1:length(mu.P))
{
  bvec <- c(1,mu.P[i])  ## constraint vector
  result <- solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2)
  sigma.P[i] <- sqrt(result$value)
  weights[i,] <- result$solution
}
sigma.mu.df <- data.frame(sigma.P = sigma.P, mu.P = mu.P )
mu.free <-  .0000822 ## input value of daily risk-free interest rate
                     ## exp(0.03 / 365) - 1 TYX 30 year CBOE yield
sharpe <- ( mu.P-mu.free) / sigma.P ## compute Sharpe's ratios
ind <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <-  (sigma.P == min(sigma.P)) ## find the minimum variance portfolio
ind3 <-  (mu.P > mu.P[ind2]) ## finally the efficient frontier
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
sigma.mu.df$col.P <- col.P
# Efficient Frontier
p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) + geom_line(aes(colour=col.P, group = col.P)) + scale_colour_identity() # + xlim(0, max(sd.R*1.1))  + ylim(0, max(mean.R)*1.1) + 
p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
options(digits=4)
p <- p + geom_abline(intercept = mu.free, slope = (mu.P[ind]-mu.free)/sigma.P[ind], colour = "red")
p <- p + geom_point(aes(x = sigma.P[ind], y = mu.P[ind])) 
p <- p + geom_point(aes(x = sigma.P[ind2], y = mu.P[ind2])) ## show min var portfolio
p <- p + annotate("text", x = sd.R[1], y = mean.R[1], label = names.R[1]) + annotate("text", x = sd.R[2], y = mean.R[2], label = names.R[2]) + annotate("text", x = sd.R[3], y = mean.R[3], label = names.R[3])
# p
ggplotly(p)
```

###No Short Sales
```{r}
# library(quadprog)
R <- returns[,1:3]/100
quantile_R <- quantile(R[,1], 0.95)
R <- subset(R, nickel > quantile_R, select = nickel:aluminium)
mean.R <- apply(R,2,mean)
cov.R <-  cov(R)
sd.R <-  sqrt(diag(cov.R))
Amat <-  cbind(rep(1,3),mean.R,diag(1,nrow=3))  # set the constraints matrix
length.P <- 300
#mu.P <- seq(0.5*min(mean.R), 1.5*max(mean.R), length = length.P)  ## set of 300 possible target portfolio returns
mu.P <-  seq(min(mean.R)+.0001,max(mean.R)-.0001,length = length.P) 
#mu.P <- seq(0.5*quantile_R, max(R), length = length.P)  ## set of 300 possible target portfolio returnssigma.P <-  mu.P # set up storage for standard deviations of portfolio returns
weights <-  matrix(0, nrow = length.P, ncol = 3) # storage for portfolio weights
for (i in 1:length(mu.P))  # find the optimal portfolios for each target expected return
{
  bvec <-  c(1,mu.P[i],rep(0,3))
  result <-  
    solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2)
  sigma.P[i] <-  sqrt(result$value)
  weights[i,] <-  result$solution
}
sigma.mu.df <- data.frame(sigma.P = sigma.P, mu.P = mu.P )
mu.free <-  .0003 ## input value of daily risk-free interest rate
sharpe <- ( mu.P-mu.free)/sigma.P ## compute Sharpe's ratios
ind <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <-  (sigma.P == min(sigma.P)) ## find the minimum variance portfolio
ind3 <-  (mu.P > mu.P[ind2]) ## finally the efficient frontier
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
sigma.mu.df$col.P <- col.P
#renderPlotly({
p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) + geom_line(aes(colour=col.P, group = col.P)) + scale_colour_identity() # + xlim(0, max(sd.R*1.1))  + ylim(0, max(mean.R)*1.1) + 
p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
options(digits=4)
p <- p + geom_abline(intercept = mu.free, slope = (mu.P[ind]-mu.free)/sigma.P[ind], colour = "red")
#p <- p + geom_point(aes(x = sigma.P[ind], y = mu.P[ind])) 
#p <- p + geom_point(aes(x = sigma.P[ind2], y = mu.P[ind2])) ## show min var portfolio
p <- p + annotate("text", x = sigma.P[ind], y = mu.P[ind], label = "T") + annotate("text", x = sigma.P[ind2], y = mu.P[ind2], label = "M") + annotate("text", x = sd.R[1], y = mean.R[1], label = names.R[1]) + annotate("text", x = sd.R[2], y = mean.R[2], label = names.R[2]) + annotate("text", x = sd.R[3], y = mean.R[3], label = names.R[3])
p <- p + geom_vline(aes(xintercept = sd.R[2]), color = "red")
p
```

###Bootstrapping a confidence interval for the Sharpe's Ratio

```{r}
# R is subset of larger R sample using u, the quantile threshold
n <-  dim(R)[1]
N <-  dim(R)[2]
mean_vect_ACTUAL <-  apply(R,2,mean)
cov_mat_ACTUAL <-  cov(R)
n_boot <-  1000
out <-  matrix(1,nrow=n_boot,ncol=2)
mean_out <-  matrix(1,nrow = n_boot,ncol = dim(R)[2])
set.seed(1016)
for (i_boot in (1:n_boot))
{
  # generate n-1 random indices
  uniform <-  ceiling((n-1)*runif(n-1))
  R_boot <-  R[uniform,] # select random sample
  mean_vect <-  apply(R_boot,2,mean)
  mean_out[i_boot,] <-  mean_vect
  cov_mat <-  cov(R_boot)
  sd_vect <-  sqrt(diag(cov_mat))
  Amat <-  cbind(rep(1,N),mean_vect) # short sales allowed
  #   Amat = cbind(rep(1,N),mean_vect,diag(1,N)) # no short sales
  sd.P <- mu.P 
  mu.P <-  seq(min(mean_vect)+.0001,max(mean_vect)-.0001,length = 300) #length.P)         
  weights <-  matrix(0,nrow=300,ncol=N) 
  for (i in 1:length(mu.P))  
  {
    bvec <-  c(1,mu.P[i])  # short sales
    #bvec = c(1,muP[i],rep(0,N)) # no short sales
    result <-  
      solve.QP(Dmat=2*cov_mat,dvec=rep(0,N),Amat=Amat,bvec=bvec,meq=2)
    sd.P[i] <-  sqrt(result$value)
    weights[i,] <-  result$solution
  } 
  sharpe <- ( mu.P-mu.free)/sd.P 
  ind <-  (sharpe == max(sharpe)) 
  out[i_boot,1] <-  sharpe[ind]
  w.T <-  weights[ind,]
  sharpe_ACTUAL <-  (w.T %*% mean_vect_ACTUAL - mu.free) / sqrt(w.T %*% cov_mat_ACTUAL %*% w.T)
  out[i_boot,2] <-  sharpe_ACTUAL
}
out_SHORT <-  data.frame(actual = out[,2], predicted = out[,1], residuals = out[,2] - out[,1])
out_SUMMARY <- data_moments(as.matrix(out_SHORT))
knitr::kable(out_SUMMARY)
```

####

```{r}
results <- out_SHORT
# Conventional approach is not very useful or even intuitive
min_xy <- min(min(results$actual), min(results$predicted))
max_xy <- max(max(results$actual), max(results$predicted))
plot_melt <- melt(results, id.vars = "predicted") #melt from reshape2 package
plot_data <- rbind(plot_melt, data.frame(predicted = c(min_xy, max_xy), variable = c("actual", "actual"), value = c(max_xy, min_xy)))
p <- ggplot(plot_data, aes(x = predicted, y = value)) + geom_point(size = 2.5) + theme_bw()
p <- p + facet_wrap(~variable, scales = "free")
p
```

###Actual vs. Predicted Sharpe's Ratios

```{r}
# Simpler scatter plots using quantiles to identify confidence intervals
p <- ggplot(results, aes(predicted, actual)) +
    geom_point() + 
    ggtitle("Actual vs. Predicted Sharpe's Ratios") + 
    geom_quantile(quantiles = c(0.01, 0.99)) + 
    geom_quantile(quantiles = 0.5, linetype = "longdash") +
    geom_density_2d(colour = "red")
p
```

###Q&A

**1.	How would the performance of these commodities affect the size and timing of shipping arrangements?**

The value at risk of the shipment mix would be matched to the market performance ergo value of the commodities. The timing of the shipment may directly correlate to the proportionate projected volatility of the shipment mix along with the projected returns versus the risk. Should that shipment contain our discovered optimal mix of all three metals, for example, shipping near a seasonal or cyclical peak of Copper, Nickel, or Aluminum would optimize shipment value while remaining diversified. It may even be necessary to inverse the proportions of shipment mix to their market performance in order to mitigate risk. In that case, more shipments would be necessary to maintain a lower value at risk while also capitalizing on pricing. The mean and standard deviations of the metals and the mixes, which could really be considered a metal index when combined, would give us a way to average out our returns and losses creating more diversity in the market holdings as well as optimize the mix we choose to purchase with our 250 million in allocated dollars.

**2.	How would the value of new shipping arrangements affect the value of our business with our current customers?**

Understandably, the ship owners would charge a proportionate or perhaps arbitrary premium for peak-pricing shipments, as there is literally more at stake - unless this sort of market timing shipment agreement was stated in the T&C or manifest.
The manufacturers would have arranged for a forwarded price in comparison to market value, so they may continue to do so paying larger strike prices, but any shipments during a commodity market cycle may be an impetus for increased forwards activity.
The traders themselves may react to increased prices with additional volumes, which tend to magnify the effect of gains or losses their brokerage fees should not change, as it is volumes that tend to benefit traders and market makers. Additionally they may be willing to pay premiums for rush shipments in order to capitalize on current or near-future pricing.

**3.	How would we manage the allocation of existing resources given we have just landed in this new market?**

We have 250 million dollars allocated and would need to determine which mix of metals we should purchase in order to maximize our returns while maintaining risk within our acceptable limit. We would use a mix of more stable metals at a higher quantity in order to minimize our volatility and exposure in the market. We also use the Sharpe ratio in order to determine our risk compared to the index of the total metals market data. The key also would be to keep the recommended amount of cash to help offset the risk in value per our risky portfolio mix. If we do not keep this amount of cash recommended then the portfolio will be too risky per our model and not be as diversified as needed in this problem. This will act as the risk free amount of money we can use for covering losses. Initially we may use derivatives (primarily options) trading in order to produce some cash flows while we attempt to place our model against current market conditions. This also gives us a chance to minimize market risk, while also understanding optimal market timing/cycle. Should our model work with the macroeconomic trends, we could then implement the shipment mix value and size while constantly updating our databases and model. Our actual vs. predicted Sharpe ratios will give us an idea of how well the returns matched our risk exposure compared to the market index. We should go for the highest Sharpe Ratio when purchasing the metals and commodities, which is a good indication of returns minus the risk free rate. If our Sharpe ratio is better than the common index, we should also be accepting more risk for loss within our portfolio. The actual vs the predicted plots show us that the value showed to be under 3 for all Shape Ratios and that the majority mix was within a quantity of 2 to 4 when we optimize the portfolio with the three metals (copper, nickel, and aluminum) in our Mu.P and sigma.P graphs that are able to be manipulated with sliders. 

It would also be wise to maintain levels of liquidity (say 5%), should there be externalities not captured in our model.

####

**Bonus:**
We discovered this piece of information while reviewing the project on https://wgfoote.shinyapps.io/extreme-4/, we did not include the answer because the codes for the weight of Markowits/QR tangency portfolio were not provided in the original homework.

![](image.png)